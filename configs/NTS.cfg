# Please fill this to your needs
# for Neural Text Simplificatio models
log_file = ${PATH_TO_LOGS}
save_model = ../models/nts
data = ${PATH_TO_DATA .t7 format}
gpuid = 1,2,...,n or 0, depending on the number of GPUs

# for word2vec models
pre_word_vecs_dec = ${WORD_EMB_DEC}
pre_word_vecs_enc = ${WORD_EMB_ENC}

global_attention = general
input_feed = 1
train_from = 
sample_w_ppl_max = -1.5
dropout_input = false
curriculum = 0
uneven_batches = false
brnn_merge = sum
dropout = 0.3
layers = 2
h = false
feat_vec_exponent = 0.7
seed = 3435
rnn_type = LSTM
async_parallel_minbatch = 1000
attention = global
exp_host = 127.0.0.1
model_type = seq2seq
max_grad_norm = 5
save_config = 
continue = false
pdbrnn = false
disable_mem_optimization = false
sample_w_ppl = false
min_learning_rate = 0
save_every = 5000
profiler = false
tgt_word_vec_size = 500
disable_logs = false
decay = default
word_vec_size = 0
param_init = 0.1
async_parallel = false
fix_word_vecs_enc = 0
exp = 
pdbrnn_reduction = 2
start_epoch = 1
log_level = INFO
src_word_vec_size = 500
optim = sgd
sample = 0
start_decay_ppl_delta = 0
fallback_to_cpu = false
start_iteration = 1
learning_rate_decay = 0.7
config = 
residual = false
fp16 = false
no_nccl = false
end_epoch = 14
feat_vec_size = 20
fix_word_vecs_dec = 0
learning_rate = 1
exp_port = 8889
feat_merge = concat
report_every = 50
brnn = false
dbrnn = false
start_decay_at = 7
sample_w_ppl_init = 15
md = false
rnn_size = 500
max_batch_size = 64

